{
  "metadata": {
    "name": "books-analysis",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, udf, when, explode, desc\nfrom pyspark.sql.types import ArrayType, StringType"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Initialize Spark Session\nspark \u003d SparkSession.builder.appName(\"ReviewAnalysis\").getOrCreate()\n\n# Read data (30000 rows of ratings with emotions)\npath \u003d \"/user/tl4151_nyu_edu/emotion_analysis_result_1209\"\nrating_df \u003d spark.read.parquet(path)\nprint(\"Number of total rows:\", rating_df.count())\nprint(\"Number of unique books:\", rating_df.select(\"Title\").distinct().count())\nrating_df.show(20)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Only keep the top/ higheest emotion of each review\nfrom pyspark.sql.functions import greatest, col, lit, when\n\n# List of emotion columns\nemotion_columns \u003d [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n\n# Add the \u0027emotion\u0027 column by finding the column with the highest score\nrating_df \u003d rating_df.withColumn(\n    \"emotion\",\n    when(col(\"sadness\") \u003d\u003d greatest(*[col(c) for c in emotion_columns]), \"sadness\")\n    .when(col(\"joy\") \u003d\u003d greatest(*[col(c) for c in emotion_columns]), \"joy\")\n    .when(col(\"love\") \u003d\u003d greatest(*[col(c) for c in emotion_columns]), \"love\")\n    .when(col(\"anger\") \u003d\u003d greatest(*[col(c) for c in emotion_columns]), \"anger\")\n    .when(col(\"fear\") \u003d\u003d greatest(*[col(c) for c in emotion_columns]), \"fear\")\n    .when(col(\"surprise\") \u003d\u003d greatest(*[col(c) for c in emotion_columns]), \"surprise\")\n)\n\n# Show a few rows to verify the new \nrating_df \u003d rating_df.select(\"row_id\", \"Title\", \"review/score\", \"review/text\", \"emotion\")\nrating_df.show(5)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Top k keywords"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Unigram"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import explode, split, col, lower, regexp_replace\n\n\n# Remove punctuation and convert to lowercase\nrating_df \u003d rating_df.withColumn(\n    \"cleaned_text\",\n    lower(regexp_replace(col(\"review/text\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))  # Keep only letters, numbers, and spaces\n)\n\n# Tokenize the review/text column\ntokenized_df \u003d rating_df.select(\n    col(\"row_id\"),\n    col(\"cleaned_text\"),\n    explode(split(col(\"cleaned_text\"), \"\\\\s+\")).alias(\"word\")\n)\n\nprint(tokenized_df.count())\ntokenized_df.show(20)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# Filter out stopwords\nuni_stop_words \u003d [\u0027a\u0027, \u0027an\u0027, \u0027the\u0027, \u0027is\u0027, \u0027in\u0027, \u0027at\u0027, \u0027of\u0027, \u0027on\u0027, \u0027and\u0027, \u0027to\u0027, \u0027for\u0027, \u0027with\u0027, \u0027this\u0027, \u0027that\u0027, \u0027it\u0027, \u0027was\u0027, \u0027as\u0027, \u0027by\u0027, \u0027are\u0027, \u0027from\u0027, \u0027be\u0027, \u0027has\u0027, \u0027had\u0027, \u0027will\u0027, \u0027would\u0027, \u0027can\u0027, \u0027could\u0027, \u0027should\u0027, \u0027you\u0027, \u0027your\u0027, \u0027we\u0027, \u0027they\u0027, \u0027their\u0027, \u0027but\u0027, \u0027not\u0027, \u0027or\u0027, \u0027if\u0027, \u0027which\u0027, \u0027when\u0027, \u0027what\u0027, \u0027how\u0027, \u0027where\u0027, \u0027if\u0027, \u0027i\u0027, \u0027book\u0027, \u0027have\u0027, \u0027read\u0027, \u0027he\u0027, \u0027his\u0027, \u0027my\u0027, \u0027one\u0027, \u0027all\u0027, \u0027about\u0027, \u0027so\u0027, \u0027who\u0027, \u0027like\u0027, \u0027her\u0027, \u0027more\u0027, \u0027very\u0027, \u0027just\u0027, \u0027some\u0027, \u0027out\u0027, \u0027there\u0027, \u0027me\u0027, \u0027she\u0027, \u0027books\u0027, \u0027really\u0027, \u0027many\u0027, \u0027up\u0027, \u0027because\u0027, \u0027into\u0027, \u0027much\u0027, \u0027no\u0027, \u0027than\u0027, \u0027most\u0027, \u0027were\u0027, \u0027been\u0027, \u0027do\u0027, \u0027also\u0027, \u0027it\u0027, \u0027dont\u0027, \u0027then\u0027, \u0027its\u0027, \u0027book\u0027, \u0027its\u0027, \u0027story\u0027, \u0027great\u0027, \u0027first\u0027, \u0027good\u0027, \u0027reading\u0027, \u0027time\u0027, \u0027only\u0027, \u0027get\u0027, \u0027other\u0027, \u0027people\u0027, \u0027even\u0027, \u0027think\u0027, \u0027book\u0027, \u0027after\u0027, \u0027any\u0027, \u0027through\u0027, \u0027way\u0027, \u0027these\u0027, \u0027well\u0027, \u0027its\u0027, \u0027know\u0027, \u0027two\u0027, \u0027am\u0027, \u0027them\u0027, \u0027make\u0027, \u0027little\u0027, \u0027find\u0027, \u0027best\u0027, \u0027never\u0027, \u0027found\u0027, \u0027still\u0027, \u0027out\u0027, \u0027every\u0027, \u0027does\u0027, \u0027did\u0027, \u0027see\u0027, \u0027want\u0027, \u0027years\u0027, \u0027ever\u0027, \u0027written\u0027, \u0027too\u0027, \u0027while\u0027, \u0027being\u0027, \u0027each\u0027, \u0027author\u0027, \u0027him\u0027, \u0027such\u0027, \u0027us\u0027, \u0027im\u0027, \u0027few\u0027, \u0027now\u0027, \u0027world\u0027, \u0027say\u0027, \u0027own\u0027, \u0027things\u0027, \u0027end\u0027, \u0027work\u0027, \u0027ive\u0027, \u0027made\u0027, \u0027must\u0027, \u0027writing\u0027, \u0027better\u0027, \u0027back\u0027, \u0027put\u0027, \u0027before\u0027, \u0027makes\u0027, \u0027last\u0027, \u0027read\u0027, \u0027lot\u0027, \u0027go\u0027, \u0027going\u0027, \u0027another\u0027, \u0027something\u0027, \u0027may\u0027, \u0027reader\u0027, \u0027thought\u0027, \u0027feel\u0027, \u0027didnt\u0027, \u0027plot\u0027, \u0027always\u0027, \u0027same\u0027, \u0027give\u0027, \u0027pages\u0027, \u0027long\u0027, \u0027youre\u0027, \u0027characters\u0027, \u0027new\u0027, \u0027series\u0027, \u0027those\u0027, \u0027our\u0027, \u0027over\u0027, \u0027character\u0027, \u0027anyone\u0027, \u0027old\u0027, \u0027interesting\u0027, \u0027got\u0027, \u0027nothing\u0027, \u0027take\u0027, \u0027why\u0027, \u0027man\u0027, \u0027real\u0027, \u0027since\u0027, \u0027both\u0027, \u0027next\u0027, \u0027cant\u0027, \u0027between\u0027, \u0027different\u0027, \u0027stories\u0027, \u0027need\u0027, \u0027however\u0027, \u0027though\u0027, \u0027down\u0027, \u0027without\u0027, \u0027part\u0027, \u0027come\u0027, \u0027easy\u0027, \u0027hard\u0027, \u0027use\u0027, \u0027almost\u0027, \u0027doesnt\u0027, \u0027seems\u0027, \u0027times\u0027, \u0027help\u0027, \u0027quite\u0027, \u0027main\u0027, \u0027until\u0027, \u0027around\u0027, \u0027yet\u0027, \u0027used\u0027, \u0027highly\u0027, \u0027might\u0027, \u0027off\u0027, \u0027worth\u0027, \u0027once\u0027, \u0027keep\u0027, \u0027thing\u0027, \u0027three\u0027, \u0027enough\u0027, \u0027far\u0027, \u0027looking\u0027, \u0027rather\u0027, \u0027story\u0027, \u0027life\u0027, \u0027time\u0027, \u0027both\u0027, \u0027trying\u0027, \u0027reading\u0027, \u0027it\u0027, \u0027enjoyed\u0027, \u0027loved\u0027, \u0027buy\u0027, \u0027actually\u0027, \u0027understand\u0027, \u0027everything\u0027, \u0027everyone\u0027, \u0027although\u0027, \u0027having\u0027, \u0027become\u0027, \u0027look\u0027, \u0027excellent\u0027, \u0027enjoy\u0027, \u0027bit\u0027, \u0027point\u0027, \u0027learn\u0027, \u0027right\u0027, \u0027human\u0027, \u0027takes\u0027, \u0027true\u0027, \u0027day\u0027, \u0027whole\u0027, \u0027able\u0027, \u0027short\u0027, \u0027school\u0027, \u0027mr\u0027, \u0027liked\u0027, \u0027left\u0027, \u0027place\u0027, \u0027seem\u0027, \u0027books\u0027, \u0027recommend\u0027, \u0027wonderful\u0027, \u0027favorite\u0027, \u0027high\u0027, \u0027probably\u0027, \u0027bought\u0027, \u0027full\u0027, \u0027important\u0027, \u0027felt\u0027, \u0027year\u0027, \u0027during\u0027, \u0027gives\u0027, \u0027here\u0027, \u0027simply\u0027, \u0027thats\u0027, \u0027getting\u0027, \u0027set\u0027, \u0027write\u0027, \u0027second\u0027, \u0027sure\u0027, \u0027start\u0027, \u0027bad\u0027, \u0027tell\u0027, \u0027fact\u0027, \u0027couldnt\u0027, \u0027along\u0027, \u0027away\u0027, \u0027me\u0027, \u0027words\u0027, \u0027less\u0027, \u0027wanted\u0027, \u00275\u0027, \u0027series\u0027, \u0027live\u0027, \u0027often\u0027, \u0027done\u0027, \u0027kind\u0027, \u0027read\u0027, \u0027try\u0027, \u0027big\u0027, \u0027believe\u0027, \u0027lives\u0027, \u0027already\u0027, \u0027came\u0027, \u0027them\u0027, \u0027said\u0027, \u0027hope\u0027, \u0027myself\u0027, \u0027goes\u0027, \u0027isnt\u0027, \u0027information\u0027, \u0027someone\u0027, \u0027them\u0027, \u0027came\u0027, \u0027several\u0027, \u0027readers\u0027, \u0027page\u0027, \u0027i\u0027, \u0027truly\u0027, \u0027especially\u0027, \u0027least\u0027, \u0027anything\u0027, \u0027comes\u0027, \u0027again\u0027, \u0027sense\u0027, \u0027money\u0027, \u0027novel\u0027, \u0027novel\u0027, \u0027one\u0027, \u0027told\u0027, \u00272\u0027, \u0027others\u0027, \u0027took\u0027, \u0027story\u0027, \u0027books\u0027, \u0027wont\u0027, \u0027wait\u0027, \u0027youll\u0027, \u0027together\u0027, \u0027lost\u0027, \u0027job\u0027, \u0027novels\u0027, \u0027care\u0027, \u0027review\u0027, \u0027small\u0027, \u0027gets\u0027, \u0027given\u0027, \u0027english\u0027, \u0027novel\u0027, \u0027making\u0027, \u0027early\u0027, \u0027let\u0027, \u0027want\u0027, \u0027doing\u0027, \u0027style\u0027, \u0027chapter\u0027, \u0027reviews\u0027, \u0027went\u0027, \u0027movie\u0027, \u0027hes\u0027, \u0027shows\u0027, \u0027mind\u0027, \u0027using\u0027, \u0027interested\u0027, \u0027definitely\u0027, \u0027id\u0027, \u0027time\u0027, \u00273\u0027, \u0027quotthe\u0027, \u0027series\u0027, \u0027me\u0027, \u0027gave\u0027, \u0027wasnt\u0027, \u0027and\u0027, \u0027ago\u0027,\n    \u0027class\u0027, \u0027works\u0027, \u0027reason\u0027, \u0027writer\u0027, \u0027past\u0027, \u0027age\u0027, \u0027kids\u0027, \u0027home\u0027, \u0027kind\u0027, \u0027himself\u0027, \u0027fun\u0027, \u0027friend\u0027, \u0027course\u0027, \u0027action\u0027, \u002710\u0027, \u0027woman\u0027, \u0027person\u0027, \u0027authors\u0027, \u0027beginning\u0027, \u0027women\u0027, \u0027men\u0027, \u0027side\u0027, \u0027mother\u0027, \u0027pretty\u0027, \u0027today\u0027, \u0027parents\u0027, \u0027events\u0027,\n    \u0027instead\u0027, \u0027ending\u0027, \u0027detail\u0027, \u0027later\u0027, \u0027son\u0027, \u0027seen\u0027, \u0027within\u0027, \u0027version\u0027, \u0027absolutely\u0027, \u0027five\u0027, \u0027either\u0027, \u0027major\u0027, \u0027daughter\u0027, \u0027knew\u0027, \u0027case\u0027, \u0027girl\u0027, \u0027original\u0027, \u0027wants\u0027, \u0027yourself\u0027, \u0027throughout\u0027, \u0027wrote\u0027, \u0027view\u0027, \u0027half\u0027, \u0027cover\u0027, \u0027ones\u0027, \u0027days\u0027,\n    \u0027called\u0027, \u0027next\u0027, \u0027thinking\u0027, \u0027text\u0027, \u0027yes\u0027, \u0027says\u0027\n    ]\n\n# Convert the stop words list to a broadcast variable for efficiency\nuni_broadcast_stop_words \u003d spark.sparkContext.broadcast(uni_stop_words)\n\n# Filter out stop words\nfiltered_df \u003d tokenized_df.filter(~col(\"word\").isin(uni_broadcast_stop_words.value))\n\nprint(filtered_df.count())\nfiltered_df.show(20)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# Count word frequencies\nkeyword_counts \u003d filtered_df.groupBy(\"word\").count()\nsorted_keywords \u003d keyword_counts.orderBy(col(\"count\").desc())\nsorted_keywords.show(100)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Bigram"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# First remove stop words from the sentence before finding bigrams\nbroadcast_uni_stop_words \u003d spark.sparkContext.broadcast(uni_stop_words)\n\ndef remove_single_stopwords(sentence):\n    if not sentence or sentence.strip() \u003d\u003d \"\":\n        return sentence\n    words \u003d sentence.split()\n    # Remove single-word stop words\n    filtered_words \u003d [word for word in words if word not in broadcast_uni_stop_words.value]\n    return \" \".join(filtered_words)\n\nremove_single_stopwords_udf \u003d udf(remove_single_stopwords, StringType())\n\n\n# Define UDF to generate bigrams\ndef generate_ngrams(text, n):\n    if not text or text.strip() \u003d\u003d \"\":  # Handle None or empty strings\n        return []\n    words \u003d text.split()\n    ngrams \u003d [\u0027 \u0027.join(words[i:i+n]) for i in range(len(words) - n + 1)]\n    return ngrams\n    \nbigrams_udf \u003d udf(lambda text: generate_ngrams(text, 2), ArrayType(StringType()))\n\n\n# Remove single-word stop words from the cleaned_text column\nrating_df \u003d rating_df.withColumn(\"cleaned_text_new\", remove_single_stopwords_udf(col(\"cleaned_text\")))\n# Generate biagrams from\nbigrams \u003d rating_df.withColumn(\"bigrams\", bigrams_udf(col(\"cleaned_text_new\")))\n\n# Explode \u0027bigrams\u0027 column to create one row per bigram\nbigrams_df \u003d bigrams.select(\n    col(\"row_id\"), \n    col(\"cleaned_text\"),\n    explode(col(\"bigrams\")).alias(\"bigram\")  # Explode the bigrams column\n)\n\nprint(bigrams_df.count())\nbigrams_df.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbigram_counts \u003d bigrams_df.groupBy(\"bigram\").count()\nsorted_bigrams \u003d bigram_counts.orderBy(col(\"count\").desc())\nsorted_bigrams.show(100, truncate\u003dFalse)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Trigrams"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# Functions to remove stopwords and generate ngram are already defined in the bigrams code, stopwords are removed in bigrams code as well\ntrigram_udf \u003d udf(lambda text: generate_ngrams(text, 3), ArrayType(StringType()))\ntrigrams \u003d rating_df.withColumn(\"trigrams\", trigram_udf(col(\"cleaned_text_new\")))\ntrigrams_df \u003d trigrams.select(\n    col(\"row_id\"),\n    col(\"cleaned_text_new\"),\n    explode(col(\"trigrams\")).alias(\"trigram\")  # Explode the trigrams column\n)\n\ntrigram_counts \u003d trigrams_df.groupBy(\"trigram\").count().orderBy(desc(\"count\"))\ntrigram_counts.show(100, truncate\u003dFalse)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Top keywords\n1. Themes/ Genres: family, history, war, spiritual, entertaining, mystery, death, vampire, gang, religion, art, fairy tale, love\n2. Authors/Characters: robert jordan, se hinton, stephen king, F. Scott Fitzgerald, anne rice, dale carnegie, helen fielding, kurt vonnegut, lemony snicket, lord rings\n3. Literary Periods/Movements: american dream, 20th century, american literature, civil war, 19th century\n4. Adjective: boring, disapointed, 4 stars, blah blah blah"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Partition by top keywords"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import col, array, when, lit, explode, expr\n\n# Step 1: Define the list of keywords\nkeywords \u003d [\n    \"family\", \"history\", \"war\", \"spiritual\", \"entertaining\", \"mystery\", \"death\", \"vampire\", \"gang\", \"religion\",\n    \"art\", \"fairy tale\", \"love\", \"robert jordan\", \"se hinton\", \"stephen king\", \"F. Scott Fitzgerald\", \"anne rice\",\n    \"dale carnegie\", \"helen fielding\", \"kurt vonnegut\", \"lemony snicket\", \"american dream\", \"20th century\",\n    \"american literature\", \"civil war\", \"19th century\", \"lord rings\", \"boring\", \"disapointed\", \"4 stars\", \"blah blah blah\"\n]\n\n# Step 2: Create a column containing an array of matched keywords\nkeywords_df \u003d rating_df.withColumn(\n    \"matched_keywords\",\n    array(*[when(col(\"cleaned_text_new\").contains(keyword), lit(keyword)) for keyword in keywords])\n)\n\n# Step 3: Remove nulls from the `matched_keywords` array\nkeywords_df \u003d keywords_df.withColumn(\n    \"matched_keywords\",\n    expr(\"filter(matched_keywords, x -\u003e x IS NOT NULL)\")\n)\n\nprint(\"Count of rows before exploding by keywords and drop rows w/o keywords\", keywords_df.count())\n\n# Step 4: Explode the `matched_keywords` column to create one row per keyword\nkeywords_df \u003d keywords_df.withColumn(\"keyword\", explode(col(\"matched_keywords\")))\n\nprint(\"Count of rows after exploding by keywords\", keywords_df.count())\nprint(\"Number of unique books with review containing top keywords\", keywords_df.select(\"Title\").distinct().count())\nkeywords_df.show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Drop duplicates and only select the needed columns\nresult_df \u003d keywords_df.dropDuplicates([\"Title\", \"review/score\", \"emotion\", \"keyword\"])\nresult_df \u003d result_df.select(\"Title\", \"review/score\", \"emotion\", \"keyword\")\nprint(\"Count of rows after droping duplicates:\", result_df.count())\nprint(\"Number of unique books with review containing top keywords\", result_df.select(\"Title\").distinct().count())\nresult_df.show(20)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Convert review/score column to float and remove invalid ones with -1\nresult_df \u003d result_df.withColumn(\n    \"review/score\",\n    when(col(\"review/score\").cast(\"float\").isNotNull(), col(\"review/score\").cast(\"float\"))\n    .otherwise(-1.0)\n)\n\n# Save the result\noutput_path \u003d \"/user/tl4151_nyu_edu/books_result_1209\"\nresult_df.write.mode(\"overwrite\").partitionBy(\"emotion\", \"keyword\", \"review/score\").parquet(output_path)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Emotion distribution"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nemotion_distribution \u003d rating_df.groupBy(\"emotion\").count().orderBy(\"count\", ascending\u003dFalse)\nemotion_distribution.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Review/Score Distribution"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import col, when\n\n# Convert review/score datatype from string to float\nrating_df \u003d rating_df.withColumn(\n    \"review/score\",\n    when(col(\"review/score\").cast(\"float\").isNotNull(), col(\"review/score\").cast(\"float\"))\n    .otherwise(-1.0)\n)\n\n# Show the result\nrating_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nreview_score_distribution \u003d rating_df.groupBy(\"review/score\").count().orderBy(\"count\", ascending\u003dFalse)\nreview_score_distribution.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Emotion vs Rating Distribution"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### 1. Distribution of Rating for each Emotion"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nrating_distribution_by_emotion \u003d result_df.groupBy(\"emotion\", \"review/score\").count().orderBy(\"emotion\", \"review/score\")\n\n# Show the results for each emotion\nemotions \u003d result_df.select(\"emotion\").distinct().collect()\nfor emotion_row in emotions:\n    emotion \u003d emotion_row[\"emotion\"]\n    print(f\"Distribution of Ratings for Emotion: {emotion}\")\n    rating_distribution_by_emotion.filter(col(\"emotion\") \u003d\u003d emotion).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### 2. Distribution of Emotion for each Rating"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nemotion_distribution_by_rating \u003d result_df.groupBy(\"review/score\", \"emotion\").count().orderBy(\"review/score\", \"emotion\")\n\n# Show the results for each rating\nratings \u003d result_df.select(\"review/score\").distinct().collect()\nfor rating_row in ratings:\n    rating \u003d rating_row[\"review/score\"]\n    print(f\"Distribution of Emotions for Rating: {rating}\")\n    emotion_distribution_by_rating.filter(col(\"review/score\") \u003d\u003d rating).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### 3. Most Frequent Top Keywords in each Emotion"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\n\n# Group by emotion and keyword to count occurrences\nkeyword_distribution \u003d result_df.groupBy(\"emotion\", \"keyword\").count()\n\n# Use a window to rank the keywords for each emotion by count\nwindow_spec \u003d Window.partitionBy(\"emotion\").orderBy(col(\"count\").desc())\n\n# Add a rank column and filter for the top 10 keywords per emotion\nmost_frequent_keywords \u003d keyword_distribution.withColumn(\n    \"rank\", row_number().over(window_spec)\n).filter(col(\"rank\") \u003c\u003d 10)\n\n# Show the top 10 keywords for each emotion\nprint(\"Top 10 Keywords for Each Emotion\")\nmost_frequent_keywords.orderBy(\"emotion\", \"rank\").show(60, truncate\u003dFalse)  # Show all 6*10\u003d60 rows"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Calculate table to plot a heapmap that shows the relationship between top keywords and ratings\nfrom pyspark.sql.functions import col, count, sum as _sum\n\n# Generate table to plot the heapmap\ntop_keywords \u003d [\n    \"family\", \"history\", \"war\", \"spiritual\", \"entertaining\", \"mystery\", \"death\", \"vampire\", \"gang\", \"religion\", \"art\", \n    \"fairy tale\", \"love\", \"robert jordan\", \"se hinton\", \"stephen king\", \"F. Scott Fitzgerald\", \"anne rice\", \"dale carnegie\", \n    \"helen fielding\", \"kurt vonnegut\", \"lemony snicket\", \"lord rings\", \"american dream\", \"20th century\", \"american literature\", \n    \"civil war\", \"19th century\", \"boring\", \"disapointed\", \"4 stars\", \"blah blah blah\"\n]\n\n# Filter the DataFrame for top keywords\nfiltered_df \u003d result_df.filter(col(\"keyword\").isin(top_keywords))\n\n# Group by keyword and review score, and calculate counts\ngrouped_df \u003d filtered_df.groupBy(\"keyword\", \"review/score\").agg(count(\"*\").alias(\"count\"))\n\n# Calculate total counts per keyword\ntotal_counts_df \u003d grouped_df.groupBy(\"keyword\").agg(_sum(\"count\").alias(\"total_count\"))\n\n# Join to calculate percentages\npercentage_df \u003d grouped_df.join(total_counts_df, \"keyword\")\npercentage_df \u003d percentage_df.withColumn(\"percentage\", (col(\"count\") / col(\"total_count\")) * 100)\npercentage_df.write.mode(\"overwrite\").csv(\"/user/tl4151_nyu_edu/heatmap_data_new.csv\", header\u003dTrue)"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\npercentage_df.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}