{
  "metadata": {
    "name": "data-merge",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\npart_file_dir \u003d \u0027/user/tw2770_nyu_edu/final-project/chunk-merged\u0027\nemotion \u003d spark.read.parquet(part_file_dir)\n\nemotion.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nemotion.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Find the emotion with the highest score"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark import StorageLevel\nfrom pyspark.sql import Window, SparkSession\nfrom pyspark.sql.functions import udf, col, regexp_replace, lower, greatest, ntile, when\nfrom pyspark.sql.types import StringType, ArrayType, StructType, StructField, FloatType\n\n\nemotion \u003d emotion.withColumn(\"max_score\", greatest(\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\")) \\\n                   .withColumn(\"emotion\",\n                               when(col(\"max_score\") \u003d\u003d col(\"sadness\"), \"sadness\")\n                               .when(col(\"max_score\") \u003d\u003d col(\"joy\"), \"joy\")\n                               .when(col(\"max_score\") \u003d\u003d col(\"love\"), \"love\")\n                               .when(col(\"max_score\") \u003d\u003d col(\"anger\"), \"anger\")\n                               .when(col(\"max_score\") \u003d\u003d col(\"fear\"), \"fear\")\n                               .when(col(\"max_score\") \u003d\u003d col(\"surprise\"), \"surprise\")\n                               .otherwise(\"unknown\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nemotion.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Drop unused columns"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ncolumns_to_drop \u003d [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\", \"max_score\"]\nemotion \u003d emotion.drop(*columns_to_drop).persist()\n\nemotion.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Calculate the distribution among the emotions"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Group by \u0027emotion\u0027 and count occurrences\nemotion_counts \u003d emotion.groupBy(\"emotion\").count()\nemotion_counts.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import col, round\n\n# Calculate total number of records\ntotal \u003d emotion.count()\n\n# Add a \u0027percentage\u0027 column\nemotion_distribution \u003d emotion_counts.withColumn(\n    \"percentage\",\n    round((col(\"count\") / total) * 100, 2)\n).orderBy(col(\"count\").desc())\n\n# Show the distribution with percentages\nemotion_distribution.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndf_path \u003d \u0027/user/tw2770_nyu_edu/final-project/lyrics_partitioned.parquet\u0027\ndf \u003d spark.read.parquet(df_path)\ndf.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Merge `emotion` data with original dataset by `id` column"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import broadcast\n\ndf_merged \u003d emotion.join(broadcast(df), on\u003d\"id\", how\u003d\"inner\").persist()\ndf_merged.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ncolumns_to_drop \u003d [\"year\", \"features\", \"language\", \"lyrics_cleaned\", \"views_partition\"]\nfinal_df\u003d df_merged.drop(*columns_to_drop).persist()\n\nfinal_df.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Create `rating` column from 1 to 10 by the percentile of `views` column"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import Window\n\nwindow_spec \u003d Window.orderBy(\"views\")\nfinal_df \u003d final_df.withColumn(\"rating\", ntile(10).over(window_spec))\n\nfinal_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfinal_df \u003d final_df.drop(\"views\").persist()\n\nfinal_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfinal_df \u003d final_df.dropDuplicates([\"id\", \"title\", \"tag\"])\nprint(\"Number of unique songs: \", final_df.count())\nfinal_df.show(20)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Partition By Top Keywords"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import col, array, when, lit, explode, expr\n\n# Step 1: Define the list of keywords\nkeywords \u003d [\n    \u0027like\u0027, \u0027life\u0027, \u0027love\u0027, \u0027yeah\u0027, \u0027shit\u0027, \u0027fuck\u0027, \u0027bitch\u0027, \u0027world\u0027, \u0027mind\u0027, \u0027heart\u0027, \u0027girl\u0027, \u0027die\u0027, \u0027money\u0027, \u0027baby\u0027, \u0027god\u0027, \u0027leave\u0027, \u0027best\u0027, \u0027alone\u0027, \u0027pain\u0027, \u0027stay\u0027, \u0027night\u0027, \u0027every night\u0027, \u0027need loving\u0027, \u0027nothing left\u0027, \u0027yo bitch\u0027, \u0027say goodbye\u0027, \u0027best friend\u0027, \u0027never ever\u0027, \u0027really want\u0027, \u0027new york\u0027, \u0027brand new\u0027, \u0027hip hop\u0027, \u0027say love\u0027, \u0027leave alone\u0027, \u0027never stop\u0027, \u0027never forget\u0027, \u0027years ago\u0027, \u0027juicy dead girls\u0027, \u0027ay ay ay\u0027, \u0027oh oh oh\u0027, \u0027na na na\u0027\n]\n\n# Step 2: Create a column containing an array of matched keywords\nkeywords_df \u003d final_df.withColumn(\n    \"matched_keywords\",\n    array(*[when(col(\"lyrics\").contains(keyword), lit(keyword)) for keyword in keywords])\n)\n\n# Step 3: Remove nulls from the `matched_keywords` array\nkeywords_df \u003d keywords_df.withColumn(\n    \"matched_keywords\",\n    expr(\"filter(matched_keywords, x -\u003e x IS NOT NULL)\")\n)\n\nprint(\"Count of rows before exploding by keywords\", keywords_df.count())\n\n# Step 4: Explode the `matched_keywords` column to create one row per keyword\nkeywords_df \u003d keywords_df.withColumn(\"keyword\", explode(col(\"matched_keywords\")))\n\nprint(\"Count of rows after exploding by keywords\", keywords_df.count())\nprint(\"Number of unique songs with lyrics containing top keywords\", keywords_df.select(\"Title\").distinct().count())\nkeywords_df.show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Drop duplicates and only select the needed columns\nresult_df \u003d keywords_df.dropDuplicates([\"id\", \"rating\", \"emotion\", \"keyword\"])\nresult_df \u003d result_df.select(\"title\", \"tag\", \"artist\", \"rating\", \"emotion\", \"keyword\")\nprint(\"Count of rows after droping duplicates:\", result_df.count())\nprint(\"Number of unique books with review containing top keywords\", result_df.select(\"Title\").distinct().count())\nresult_df.show(20)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\noutput_dir \u003d \u0027/user/tw2770_nyu_edu/final-project/lyrics-emotion-keyword-rating\u0027\n\nresult_df.write \\\n    .partitionBy(\"emotion\", \"keyword\", \"rating\") \\\n    .mode(\"overwrite\") \\\n    .option(\"compression\", \"snappy\") \\\n    .parquet(output_dir)"
    }
  ]
}