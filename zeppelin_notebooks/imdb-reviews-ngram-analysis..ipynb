{
  "metadata": {
    "name": "imdb-reviews-ngram-analysis",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# IMDB Reviews Keyword Extraction Using Scala\n\nAuthor: You-Jun Chen (yc7093)\n\n## 1. Introduction\n\nThis Zeppelin notebook demonstrates the ingestion and processing of the [IMDB Review Dataset](https://www.kaggle.com/datasets/ebiswas/imdb-review-dataset?resource\u003ddownload), sourced from Kaggle.\nThe dataset contains user reviews of movies and TV shows. It includes fields such as `review_id`, `review_summary`, `review_detail`, `rating`, and more. The dataset is split into six JSON files, each approximately 1.5GB in size. This notebook processes a single file, `part-01.json`, as a demonstration. The methods shown can be applied to the remaining files.\n\nThe primary objective of the data ingestion process is to partition the dataset by rating and keywords. This allows efficient access to subsets of the data for targeted analysis. For instance, we can quickly retrieve records with a specific rating and keyword using the following partition path:\n\n```scala\nval specificPartitionPath \u003d \"/user/yc7093_nyu_edu/imdb_partitioned_by_rating_and_words_parquet/rating\u003d9.0/word\u003dfun_watch\"\n```\n\n### Challenges\nInitally, I encountered issues loading the JSON files directly into Spark. The error was likely due to non-standard formatting in the JSON file. \nI attempted debugging but could not resolve the issue before the deadline.\n\n\n### Workaround\n\nTo overcome this challenge, I used Pandas to preprocess the data:\n1. Converted the problematic JSON file into parquet format.\n2. Used the resulting parquet file (`part-01.parquet`) as input to the Spark pipeline in this notebook.\n\nHere is the Python code used for preprocessing:\n\n```python\nimport pandas as pd\n\ndf \u003d pd.read_json(\"part-01.json\")\n\ndf.to_parquet(\"~/part-01.parquet\", index\u003dFalse)\n```\n\nI will investigate further to identify and resolve the issues with the JSON format for future scalability. For now, the CSV file serves as a clean and manageable input to proceed with data ingestion and transformation.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Load Data\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val basePath \u003d \"/user/yc7093_nyu_edu/imdb-reviews-w-emotion/part\"\nval fileSuffixes \u003d List(\"-01-all\") //, \"-02-all\", \"-03-all\", \"-04-all\")\n\nval initialPath \u003d s\"$basePath${fileSuffixes.head}\"\nvar rawDF \u003d spark.read.parquet(initialPath)\n\nfor (suffix \u003c- fileSuffixes.tail) {\n  val fullPath \u003d s\"$basePath$suffix\" \n  val part_df \u003d spark.read.parquet(fullPath) \n  rawDF \u003d rawDF.union(part_df) \n}\n\n\nrawDF.show(5)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "rawDF.count "
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "rawDF.select(\"movie\").distinct().count()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Drop redundant columns"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df \u003d rawDF.drop(\"spoiler_tag\", \"helpful\", \"predicted_emotion\")\n\ndf.show(5)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Process the ratings\nRemove records with invalid ratings"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val distinctRatings \u003d df.select(\"rating\").distinct()\n\ndistinctRatings.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val ratingsDf \u003d df.filter(col(\"rating\").between(0, 10.0))\n\nratingsDf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "ratingsDf.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "ratingsDf.select(\"rating\").distinct().show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n## 5. Process the keywords in the reviews\n\n### Unigrams analysis\nObjective: Identify the most common words in `review_summary` and `review_detail` while removing stop words."
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\nval stopWords \u003d Set(\"a\", \"an\", \"the\", \"and\", \"or\", \"of\", \"to\", \"in\", \"is\", \"on\", \"with\", \"for\", \"it\", \"at\", \"by\", \"this\", \"that\", \"i\", \"you\", \"he\", \"she\", \"we\", \"they\", \"be\", \"was\", \"were\", \"been\", \"but\", \"if\", \"then\", \"so\", \"no\", \"yes\", \"not\", \"am\", \"are\", \"as\", \"do\", \"does\", \"did\", \"my\", \"your\", \"our\", \"their\", \"who\", \"what\", \"which\", \"how\", \"me\", \"us\", \"them\", \"about\", \"movie\", \"film\", \"films\", \"from\", \"one\", \"all\", \"have\", \"his\", \"her\", \"just\", \"more\", \"very\", \"t\", \"s\", \"story\", \"show\", \"out\", \"can\", \"than\", \"much\", \"don\", \"its\", \"ever\", \"too\", \"series\", \"will\", \"see\", \"when\", \"episode\", \"would\", \"get\", \"even\", \"only\", \"still\", \"movies\", \"into\", \"characters\", \"review\", \"make\", \"seen\", \"plot\", \"character\", \"after\", \"why\", \"also\", \"another\", \"end\", \"watching\", \"man\", \"over\", \"drama\", \"because\", \"should\", \"time\", \"watch\", \"has\", \"there\", \"here\", \"some\", \"made\", \"where\", \"him\", \"tv\", \"could\", \"many\", \"m\", \"1\", \"way\", \"ve\", \"2\", \"3\" )\n\nval broadcastStopWords \u003d spark.sparkContext.broadcast(stopWords)\n\n\nval tokenizeAndFilter \u003d udf { (text: String) \u003d\u003e\n  if (text \u003d\u003d null) Array.empty[String]\n  else {\n    text.toLowerCase\n      .split(\"\\\\W+\") // Split by non-word characters\n      .filter(word \u003d\u003e word.nonEmpty \u0026\u0026 !broadcastStopWords.value.contains(word)) // Remove stop words and empty strings\n  }\n}\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Show top 100 words in `review_summary`"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val tokenizedSummaryDf \u003d ratingsDf\n  .withColumn(\"summary_tokens\", explode(tokenizeAndFilter(col(\"review_summary\")))) // Tokenize and explode\n\nval summaryWordCounts \u003d tokenizedSummaryDf\n  .groupBy(\"summary_tokens\")\n  .count()\n  .orderBy(desc(\"count\")) \n  .withColumnRenamed(\"summary_tokens\", \"word\") \n\nprintln(\"Top Frequent Words in review_summary:\")\nsummaryWordCounts.show(100, truncate \u003d false)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Show top 100 words in `review_detail`"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Tokenize and flatten `review_detail`\nval tokenizedDetailDf \u003d ratingsDf\n  .withColumn(\"detail_tokens\", explode(tokenizeAndFilter(col(\"review_detail\")))) \n\n\nval detailWordCounts \u003d tokenizedDetailDf\n  .groupBy(\"detail_tokens\")\n  .count()\n  .orderBy(desc(\"count\")) \n  .withColumnRenamed(\"detail_tokens\", \"word\") \n\n\nprintln(\"Top Frequent Words in review_detail:\")\ndetailWordCounts.show(100, truncate \u003d false)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val unigramTargetWords \u003d List(\n    \"good\", \"great\", \"best\", \"love\", \"bad\", \"funny\", \"fun\", \"amazing\", \"worst\", \"comedy\", \"excellent\", \"boring\", \"horror\", \"entertaining\",\n    \"beautiful\", \"masterpiece\", \"brilliant\", \"classic\", \"interesting\", \"awesome\", \"terrible\", \"perfect\", \"enjoyable\", \"original\", \"fantastic\", \"wonderful\", \"horrible\",\n    \"disappointing\", \"underrated\", \"family\"\n)\n\n    \nunigramTargetWords.size"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n### Bigrams Analysis\n\nIn my initial exploration, I observed that analyzing individual words (unigrams) did not yield sufficient context or meaningful insights about the dataset. Many words appeared frequently but lacked the ability to convey the relationships or patterns within the reviews.\n\nTo address this, I utilized the **NGram** model to generate **bigrams** (two-word sequences). This approach captures relationships between adjacent words and provides richer insights into common phrases used in the reviews. For example, phrases like \"great acting\" or \"bad movie\" offer more actionable information than the individual words \"great\" or \"bad.\"\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Show top 100 bigrams in `review_summary`"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.feature.NGram\nimport org.apache.spark.sql.functions._\n\n// Define stop words\nval stopWords \u003d Set(\"a\", \"an\", \"the\", \"and\", \"or\", \"of\", \"to\", \"in\", \"is\", \"on\", \"with\", \"for\", \"it\", \"at\", \"by\", \"this\", \"that\", \"i\", \"you\", \"he\", \"she\", \"we\", \"they\", \"be\", \"was\", \"were\", \"been\", \"but\", \"if\", \"then\", \"no\", \"yes\", \"not\", \"am\", \"are\", \"as\", \"do\", \"does\", \"did\", \"my\", \"your\", \"our\", \"their\", \"who\", \"what\", \"which\", \"how\", \"me\", \"us\", \"them\", \"every\", \"set\", \"up\", \"there\", \"each\", \"feel like\", \"felt like\", \"feels like\", \"m\", \"has\", \"look like\", \"seems like\", \"could ve\")\n\nval broadcastStopWords \u003d sc.broadcast(stopWords)\n\n\nval tokenizedSummaryDf \u003d ratingsDf.withColumn(\"summary_tokens\", tokenizeAndFilter(col(\"review_summary\")))\n\nval nGramSummary \u003d new NGram()\n  .setN(2)\n  .setInputCol(\"summary_tokens\")\n  .setOutputCol(\"summary_bigrams\")\n\nval bigramSummaryDf \u003d nGramSummary.transform(tokenizedSummaryDf)\n\nval explodedSummaryBigrams \u003d bigramSummaryDf.withColumn(\"summary_bigram\", explode(col(\"summary_bigrams\")))\n\nval filteredSummaryBigrams \u003d explodedSummaryBigrams.filter { row \u003d\u003e\n  val bigram \u003d row.getString(row.fieldIndex(\"summary_bigram\"))\n  val words \u003d bigram.split(\" \")\n  words.forall(word \u003d\u003e !broadcastStopWords.value.contains(word))\n}\n\nval summaryBigramCounts \u003d filteredSummaryBigrams\n  .groupBy(\"summary_bigram\")\n  .count()\n  .orderBy(desc(\"count\"))\n\nprintln(\"Top Frequent Bigrams in review_summary:\")\nsummaryBigramCounts.show(100, truncate \u003d false)\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Show top 100 bigrams in `review_detail`"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.feature.NGram\nimport org.apache.spark.sql.functions._\n\nval tokenizedDetailDf \u003d ratingsDf.withColumn(\"detail_tokens\", tokenizeAndFilter(col(\"review_detail\")))\n\n// Generate bigrams for `review_detail`\nval nGramDetail \u003d new NGram()\n  .setN(2)\n  .setInputCol(\"detail_tokens\")\n  .setOutputCol(\"detail_bigrams\")\n\nval bigramDetailDf \u003d nGramDetail.transform(tokenizedDetailDf)\n\nval explodedDetailBigrams \u003d bigramDetailDf.withColumn(\"detail_bigram\", explode(col(\"detail_bigrams\")))\n\nval filteredDetailBigrams \u003d explodedDetailBigrams.filter { row \u003d\u003e\n  val bigram \u003d row.getString(row.fieldIndex(\"detail_bigram\"))\n  val words \u003d bigram.split(\" \")\n  words.forall(word \u003d\u003e !broadcastStopWords.value.contains(word))\n}\n\nval detailBigramCounts \u003d filteredDetailBigrams\n  .groupBy(\"detail_bigram\")\n  .count()\n  .orderBy(desc(\"count\"))\n\nprintln(\"Top Frequent Bigrams in review_detail:\")\ndetailBigramCounts.show(100, truncate \u003d false)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val bigramTargetWords \u003d List(\n  \"sci fi\", \"well done\", \"really good\", \"better expected\", \"low budget\", \"feel good\", \"surprisingly good\", \"thought provoking\", \"really bad\", \"let down\", \"good acting\", \"bad acting\", \"worth seeing\", \"waste money\", \"well worth\", \"science fiction\",\n  \"well acted\", \"action packed\", \"mind blowing\", \"romantic comedy\", \"great cast\", \"special effects\", \"good fun\", \"nothing special\", \"really enjoyed\",\n  \"action flick\", \"good idea\", \"rip off\", \"wow wow\", \"best horror\", \"rom com\", \"cult classic\", \"nothing new\", \"above average\", \"soap opera\", \"high school\",\n  \"hear warming\", \"top notch\", \"definitely worth\", \"visually stunning\", \"best action\", \"horror flick\", \"die hard\", \"pleasantly surprised\",\n  \"absolutely amazing\", \"hidden gem\", \"great family\", \"highly recommend\"\n)\n\n\n    \nbigramTargetWords.size"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Trigrams\n\nUpon analyzing the trigrams, I found that they do not provide additional meaningful keywords beyond what is already captured by bigrams. Most of the significant phrases are sufficiently represented in the bigrams, making trigrams redundant for this analysis. Therefore, I chose to focus on bigrams for extracting useful insights. \n\n\n\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val stopWords \u003d Set(\"a\", \"an\", \"the\", \"and\", \"or\", \"of\", \"to\", \"in\", \"is\", \"on\", \"with\", \"for\", \"it\", \"at\", \"by\", \"this\", \"that\", \"i\", \"you\", \"he\", \"she\", \"we\", \"they\", \"be\", \"was\", \"were\", \"been\", \"but\", \"if\", \"then\", \"no\", \"yes\", \"not\", \"am\", \"are\", \"as\", \"do\", \"does\", \"did\", \"my\", \"your\", \"our\", \"their\", \"who\", \"what\", \"which\", \"how\", \"me\", \"us\", \"them\", \"every\", \"set\", \"up\", \"there\", \"each\", \"feel like\", \"felt like\", \"feels like\", \"m\", \"has\", \"look like\", \"seems like\", \"could ve\")\n\nval broadcastStopWords \u003d spark.sparkContext.broadcast(stopWords)\n\nval tokenizedDf \u003d ratingsDf.withColumn(\"tokens\", tokenizeAndFilter(col(\"review_detail\")))\n\nval nGram \u003d new NGram()\n  .setN(3) // Set n\u003d3 for trigrams\n  .setInputCol(\"tokens\")\n  .setOutputCol(\"trigrams\")\n\nval trigramDf \u003d nGram.transform(tokenizedDf)\n\nval explodedTrigrams \u003d trigramDf.withColumn(\"trigram\", explode(col(\"trigrams\")))\n\nval filteredTrigrams \u003d explodedTrigrams.filter { row \u003d\u003e\n  val trigram \u003d row.getString(row.fieldIndex(\"trigram\"))\n  val words \u003d trigram.split(\" \")\n  words.forall(word \u003d\u003e !broadcastStopWords.value.contains(word)) // All words must not be stop words\n}\n\nval trigramCounts \u003d filteredTrigrams\n  .groupBy(\"trigram\")\n  .count()\n  .orderBy(desc(\"count\"))\n\ntrigramCounts.show(100, truncate \u003d false)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Save data as Parquet Partitioned by Rating and Word\n\nFrom the analysis above, I identified a set of keywords that are frequently used in the reviews. These keywords represent various sentiments or aspects of the movies and TV shows, such as positive adjectives (e.g., \"good\", \"great\", \"amazing\"), negative adjectives (e.g., \"bad\", \"terrible\", \"awful\"), and thematic phrases (e.g., \"special effects\", \"sci-fi\", \"roller coaster ride\"). \n\n### Target Keywords\nThe following list of target keywords was used to flag reviews based on their content.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val targetWords \u003d unigramTargetWords ++ bigramTargetWords\n\ntargetWords"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "targetWords.size"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Each review was checked for the presence of the keywords in both review_summary and review_detail.\nIf a keyword was found, a flag was added to indicate its presence, and the keyword was formatted to replace spaces with underscores for ease of partitioning."
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\nval dfWithWordFlags \u003d targetWords.foldLeft(df) { (tempDf, word) \u003d\u003e\n  tempDf.withColumn(word.replaceAll(\" \", \"_\"), \n    lower(col(\"review_detail\")).contains(word) || lower(col(\"review_summary\")).contains(word)\n  )\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val explodedDf \u003d dfWithWordFlags.selectExpr(\n  \"review_id\",\n  \"movie\",\n  \"review_summary\",\n  \"review_detail\",\n  \"emotion\",\n  \"rating\",\n  \"stack(\" + targetWords.length + \", \" +\n  targetWords.map(word \u003d\u003e s\"\u0027$word\u0027, ${word.replaceAll(\" \", \"_\")}\").mkString(\", \") +\n  \") as (word, is_present)\"\n).filter(col(\"is_present\"))\n\nexplodedDf.show(5)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// explodedDf.write.mode(\"overwrite\").parquet(\"/user/yc7093_nyu_edu/imdb-all-w-emotion-keyword-part1-4\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Saving the data in parquet format"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Save the DataFrame partitioned by `rating` and modified `word` as Parquet\nexplodedDf\n    .withColumn(\"word\", regexp_replace(col(\"word\"), \" \", \"_\")) // Replace spaces with underscores in `word`\n    .write\n    .mode(\"overwrite\") // Overwrite existing data\n    .partitionBy(\"emotion\", \"rating\", \"word\") // \n    .parquet(\"/user/yc7093_nyu_edu/imdb_partitioned_by_emotion_rating_word\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n## 6. Load the partition for testing\nAfter partitioning the dataset by `rating` and `keyword`, the partitions can be loaded selectively for further analysis.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.hadoop.fs.{FileSystem, Path}\n\n// Specify the path you want to list\nval hdfsPath \u003d new Path(\"/user/yc7093_nyu_edu/imdb_partitioned_by_emotion_rating_word/emotion\u003danger/rating\u003d9.0\")\n\n// Get the FileSystem object\nval fs \u003d FileSystem.get(spark.sparkContext.hadoopConfiguration)\n\n// List files and directories in the specified path\nval files \u003d fs.listStatus(hdfsPath)\nfiles.foreach(file \u003d\u003e println(file.getPath.toString))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Below is an example of loading a specific partition based on `rating\u003d9.0` and `word\u003dfun_watch`."
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val specificPartitionPath \u003d \"/user/yc7093_nyu_edu/imdb_partitioned_by_emotion_rating_word/emotion\u003danger/rating\u003d9.0/word\u003dhidden_gem\"\nval specificPartitionDf \u003d spark.read.parquet(specificPartitionPath)\n\n// Show the data\nspecificPartitionDf.show() // Use truncate \u003d false to see full text\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val outputPath \u003d \"/user/yc7093_nyu_edu/rating_9_fun_watch_partition\"\n\n// Write the DataFrame to HDFS as Parquet\nspecificPartitionDf.write\n  .mode(\"overwrite\") // Overwrite if the path already exists\n  .parquet(outputPath)\n\nprintln(s\"Partition saved successfully to HDFS at $outputPath\")\n"
    }
  ]
}