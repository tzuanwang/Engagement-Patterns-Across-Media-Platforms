{
  "metadata": {
    "name": "keywords-selection",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfile_path \u003d \u0027/user/tw2770_nyu_edu/final-project/lyrics_final.parquet\u0027\n\ndf \u003d spark.read.parquet(file_path)\ndf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.printSchema()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Top Keywords"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Romove punctuation of `lyrics` column"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import Tokenizer, NGram\nfrom pyspark.sql.functions import explode, col, lower, regexp_replace\n\n# Remove punctuations and convert text to lowercase\ndf \u003d df.withColumn(\"lyrics\", regexp_replace(lower(col(\"lyrics\")), r\u0027[^\\w\\s]\u0027, \u0027\u0027))"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Split `lyrics` column into tokens"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Tokenize the lyrics into individual words\ntokenizer \u003d Tokenizer(inputCol\u003d\"lyrics\", outputCol\u003d\"words\")\nwords_data \u003d tokenizer.transform(df).persist()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nwords_data.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Unigram"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.ml.feature import StopWordsRemover, Tokenizer\nfrom pyspark.sql.functions import col, explode, concat_ws\n\n\ndefault_stop_words \u003d StopWordsRemover.loadDefaultStopWords(\"english\")\nunigram_stop_words \u003d [\"   \", \u0027im\u0027, \u0027dont\u0027, \u0027know\u0027, \u0027got\u0027, \u0027get\u0027, \u0027time\u0027, \u0027one\u0027, \u0027cant\u0027, \u0027see\u0027, \u0027de\u0027, \u0027way\u0027, \u0027take\u0027, \u0027come\u0027, \u0027aint\u0027, \u0027youre\u0027, \u0027la\u0027, \u0027ill\u0027, \u0027that\u0027, \u0027think\u0027, \u0027let\u0027, \u0027man\u0027, \u0027que\u0027, \u0027back\u0027, \u0027thats\u0027, \u0027feel\u0027, \u0027cause\u0027, \u0027still\u0027, \u0027day\u0027, \u0027away\u0027, \u0027always\u0027, \u0027ive\u0027, \u0027people\u0027, \u0027going\u0027, \u0027said\u0027, \u0027keep\u0027, \u0027niggas\u0027, \u0027fucking\u0027, \u0027nigga\u0027]\nextend_uni_stop_words \u003d default_stop_words + unigram_stop_words"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(default_stop_words)"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(extend_uni_stop_words)"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Remove stop words using the custom list\nremover \u003d StopWordsRemover(inputCol\u003d\"words\", outputCol\u003d\"filtered_words\")\nremover.setStopWords(extend_uni_stop_words)\nunigram_data \u003d remover.transform(words_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nunigram_data.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Calculate unigram frequency\nunigrams_flattened \u003d unigram_data.select(explode(col(\"filtered_words\")).alias(\"unigram\"))\nunigram_counts \u003d unigrams_flattened.groupBy(\"unigram\").count()\nunigram_counts_sorted \u003d unigram_counts.orderBy(col(\"count\").desc())\nunigram_counts_sorted.show(100)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nuni_words \u003d [\u0027like\u0027, \u0027life\u0027, \u0027love\u0027, \u0027yeah\u0027, \u0027shit\u0027, \u0027fuck\u0027, \u0027bitch\u0027, \u0027world\u0027, \u0027mind\u0027, \u0027heart\u0027, \u0027girl\u0027, \u0027die\u0027, \u0027money\u0027, \u0027baby\u0027, \u0027god\u0027, \u0027leave\u0027, \u0027best\u0027, \u0027alone\u0027, \u0027pain\u0027, \u0027stay\u0027, \u0027night\u0027, \u0027best\u0027]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Bigram"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nbigram_stop_words \u003d [\"   \", \u0027im\u0027, \u0027dont\u0027, \u0027know\u0027, \u0027got\u0027, \u0027get\u0027, \u0027time\u0027, \u0027one\u0027, \u0027cant\u0027, \u0027see\u0027, \u0027de\u0027, \u0027way\u0027, \u0027take\u0027, \u0027come\u0027, \u0027aint\u0027, \u0027youre\u0027, \u0027la\u0027, \u0027ill\u0027, \u0027that\u0027, \u0027think\u0027, \u0027let\u0027, \u0027man\u0027, \u0027que\u0027, \u0027back\u0027, \u0027thats\u0027, \u0027feel\u0027, \u0027cause\u0027, \u0027still\u0027, \u0027day\u0027, \u0027away\u0027, \u0027always\u0027, \u0027ive\u0027, \u0027people\u0027, \u0027going\u0027, \u0027said\u0027, \u0027keep\u0027, \u0027niggas\u0027, \u0027fucking\u0027, \u0027interview\u0027, \u0027nigga\u0027, \u0027looks\u0027, \u0027look\u0027]\nextend_bi_stop_words \u003d default_stop_words + bigram_stop_words"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nwords_data.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Remove stop words using the custom list\nremover \u003d StopWordsRemover(inputCol\u003d\"words\", outputCol\u003d\"filtered_words\")\nremover.setStopWords(extend_bi_stop_words)\nbigrams_data \u003d remover.transform(words_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbigrams_data.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbigram \u003d NGram(n\u003d2, inputCol\u003d\"filtered_words\", outputCol\u003d\"bigrams\")\nbigram_data \u003d bigram.transform(bigrams_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbigram_data.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Explode the bigrams list to get individual bigrams as rows\nbigrams_flattened \u003d bigram_data.select(explode(col(\"bigrams\")).alias(\"bigram\"))\n# Calculate frequency by grouping by bigram and counting the occurrences\nbigram_counts \u003d bigrams_flattened.groupBy(\"bigram\").count()\n# Sort the bigrams in descending order based on count\nbigram_counts_sorted \u003d bigram_counts.orderBy(col(\"count\").desc())\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbigram_counts_sorted.show(100)"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbi_words \u003d [\u0027every night\u0027, \u0027need loving\u0027, \u0027nothing left\u0027, \u0027yo bitch\u0027, \u0027say goodbye\u0027, \u0027best friend\u0027, \u0027never ever\u0027, \u0027really want\u0027, \u0027new york\u0027, \u0027brand new\u0027, \u0027hip hop\u0027, \u0027say love\u0027, \u0027leave alone\u0027, \u0027never stop\u0027, \u0027never forget\u0027, \u0027years ago\u0027]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Trigram"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ntrigram_stop_words \u003d [\"   \", \u0027im\u0027, \u0027dont\u0027, \u0027know\u0027, \u0027got\u0027, \u0027get\u0027, \u0027time\u0027, \u0027one\u0027, \u0027cant\u0027, \u0027see\u0027, \u0027de\u0027, \u0027way\u0027, \u0027take\u0027, \u0027come\u0027, \u0027aint\u0027, \u0027youre\u0027, \u0027la\u0027, \u0027ill\u0027, \u0027that\u0027, \u0027think\u0027, \u0027let\u0027, \u0027man\u0027, \u0027que\u0027, \u0027back\u0027, \u0027thats\u0027, \u0027feel\u0027, \u0027cause\u0027, \u0027still\u0027, \u0027day\u0027, \u0027away\u0027, \u0027always\u0027, \u0027ive\u0027, \u0027people\u0027, \u0027going\u0027, \u0027said\u0027, \u0027keep\u0027, \u0027niggas\u0027, \u0027fucking\u0027, \u0027interview\u0027, \u0027nigga\u0027, \u0027looks\u0027, \u0027look\u0027]\nextend_tri_stop_words \u003d default_stop_words + trigram_stop_words"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# Remove stop words using the custom list\nremover \u003d StopWordsRemover(inputCol\u003d\"words\", outputCol\u003d\"filtered_words\")\nremover.setStopWords(extend_tri_stop_words)\ntrigrams_data \u003d remover.transform(words_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntrigrams_data.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Create trigrams using NGram\ntrigram \u003d NGram(n\u003d3, inputCol\u003d\"filtered_words\", outputCol\u003d\"trigrams\")\ntrigram_data \u003d trigram.transform(trigrams_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntrigram_data.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Explode the trigrams list to get individual trigrams as rows\ntrigrams_flattened \u003d trigram_data.select(explode(col(\"trigrams\")).alias(\"trigram\"))\n# Calculate frequency by grouping by trigram and counting the occurrences\ntrigram_counts \u003d trigrams_flattened.groupBy(\"trigram\").count()\n# Sort the trigrams in descending order based on count\ntrigram_counts_sorted \u003d trigram_counts.orderBy(col(\"count\").desc())"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntrigram_counts_sorted.show(100)"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntri_words \u003d [\u0027juicy dead girls\u0027, \u0027ay ay ay\u0027, \u0027oh oh oh\u0027, \u0027na na na\u0027]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Top Keywords\n1. Emotion and Relationships: like, life, love, yeah, mind, heart, girl, die, baby, god, leave, best friend, say goodbye, really want, say love, leave alone, never stop, never forget\n2. Hardship and Negativity: shit, fuck, bitch, world, pain, alone, nothing left, yo bitch, years ago\n3. Lifestyle and Culture: money, stay, night, every night, need loving, new york, brand new, hip hop, juicy dead girls, ay ay ay, oh oh oh, na na na\n"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nkeywords \u003d uni_words + bi_words + tri_words\nprint(keywords)"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nlen(keywords)"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}